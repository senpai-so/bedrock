'use strict';

Object.defineProperty(exports, '__esModule', { value: true });

var dagPB = require('@ipld/dag-pb');
var cid = require('multiformats/cid');
var debug = require('debug');
var ipfsUnixfs = require('ipfs-unixfs');
var dirSharded = require('./dir-sharded.js');
var hamtUtils = require('./hamt-utils.js');
var errCode = require('err-code');
var last = require('it-last');

function _interopDefaultLegacy (e) { return e && typeof e === 'object' && 'default' in e ? e : { 'default': e }; }

function _interopNamespace(e) {
  if (e && e.__esModule) return e;
  var n = Object.create(null);
  if (e) {
    Object.keys(e).forEach(function (k) {
      if (k !== 'default') {
        var d = Object.getOwnPropertyDescriptor(e, k);
        Object.defineProperty(n, k, d.get ? d : {
          enumerable: true,
          get: function () { return e[k]; }
        });
      }
    });
  }
  n["default"] = e;
  return Object.freeze(n);
}

var dagPB__namespace = /*#__PURE__*/_interopNamespace(dagPB);
var debug__default = /*#__PURE__*/_interopDefaultLegacy(debug);
var errCode__default = /*#__PURE__*/_interopDefaultLegacy(errCode);
var last__default = /*#__PURE__*/_interopDefaultLegacy(last);

const log = debug__default["default"]('ipfs:mfs:core:utils:add-link');
async function addLink(context, options) {
  let parent = options.parent;
  if (options.parentCid) {
    const parentCid = cid.CID.asCID(options.parentCid);
    if (parentCid === null) {
      throw errCode__default["default"](new Error('Invalid CID passed to addLink'), 'EINVALIDPARENTCID');
    }
    if (parentCid.code !== dagPB__namespace.code) {
      throw errCode__default["default"](new Error('Unsupported codec. Only DAG-PB is supported'), 'EINVALIDPARENTCID');
    }
    log(`Loading parent node ${ parentCid }`);
    const block = await context.repo.blocks.get(parentCid);
    parent = dagPB__namespace.decode(block);
  }
  if (!parent) {
    throw errCode__default["default"](new Error('No parent node or CID passed to addLink'), 'EINVALIDPARENT');
  }
  if (!options.cid) {
    throw errCode__default["default"](new Error('No child cid passed to addLink'), 'EINVALIDCHILDCID');
  }
  if (!options.name) {
    throw errCode__default["default"](new Error('No child name passed to addLink'), 'EINVALIDCHILDNAME');
  }
  if (!options.size && options.size !== 0) {
    throw errCode__default["default"](new Error('No child size passed to addLink'), 'EINVALIDCHILDSIZE');
  }
  if (!parent.Data) {
    throw errCode__default["default"](new Error('Parent node with no data passed to addLink'), 'ERR_INVALID_PARENT');
  }
  const meta = ipfsUnixfs.UnixFS.unmarshal(parent.Data);
  if (meta.type === 'hamt-sharded-directory') {
    log('Adding link to sharded directory');
    return addToShardedDirectory(context, {
      ...options,
      parent
    });
  }
  if (parent.Links.length >= options.shardSplitThreshold) {
    log('Converting directory to sharded directory');
    return convertToShardedDirectory(context, {
      ...options,
      parent,
      mtime: meta.mtime,
      mode: meta.mode
    });
  }
  log(`Adding ${ options.name } (${ options.cid }) to regular directory`);
  return addToDirectory(context, {
    ...options,
    parent
  });
}
const convertToShardedDirectory = async (context, options) => {
  const result = await hamtUtils.createShard(context, options.parent.Links.map(link => ({
    name: link.Name || '',
    size: link.Tsize || 0,
    cid: link.Hash
  })).concat({
    name: options.name,
    size: options.size,
    cid: options.cid
  }), options);
  log(`Converted directory to sharded directory ${ result.cid }`);
  return result;
};
const addToDirectory = async (context, options) => {
  const parentLinks = options.parent.Links.filter(link => {
    return link.Name !== options.name;
  });
  parentLinks.push({
    Name: options.name,
    Tsize: options.size,
    Hash: options.cid
  });
  if (!options.parent.Data) {
    throw errCode__default["default"](new Error('Parent node with no data passed to addToDirectory'), 'ERR_INVALID_PARENT');
  }
  const node = ipfsUnixfs.UnixFS.unmarshal(options.parent.Data);
  let data;
  if (node.mtime) {
    const ms = Date.now();
    const secs = Math.floor(ms / 1000);
    node.mtime = {
      secs: secs,
      nsecs: (ms - secs * 1000) * 1000
    };
    data = node.marshal();
  } else {
    data = options.parent.Data;
  }
  options.parent = dagPB__namespace.prepare({
    Data: data,
    Links: parentLinks
  });
  const hasher = await context.hashers.getHasher(options.hashAlg);
  const buf = dagPB__namespace.encode(options.parent);
  const hash = await hasher.digest(buf);
  const cid$1 = cid.CID.create(options.cidVersion, dagPB__namespace.code, hash);
  if (options.flush) {
    await context.repo.blocks.put(cid$1, buf);
  }
  return {
    node: options.parent,
    cid: cid$1,
    size: buf.length
  };
};
const addToShardedDirectory = async (context, options) => {
  const {shard, path} = await addFileToShardedDirectory(context, options);
  const result = await last__default["default"](shard.flush(context.repo.blocks));
  if (!result) {
    throw new Error('No result from flushing shard');
  }
  const block = await context.repo.blocks.get(result.cid);
  const node = dagPB__namespace.decode(block);
  const parentLinks = options.parent.Links.filter(link => {
    return (link.Name || '').substring(0, 2) !== path[0].prefix;
  });
  const newLink = node.Links.find(link => (link.Name || '').substring(0, 2) === path[0].prefix);
  if (!newLink) {
    throw new Error(`No link found with prefix ${ path[0].prefix }`);
  }
  parentLinks.push(newLink);
  return hamtUtils.updateHamtDirectory(context, parentLinks, path[0].bucket, options);
};
const addFileToShardedDirectory = async (context, options) => {
  const file = {
    name: options.name,
    cid: options.cid,
    size: options.size
  };
  if (!options.parent.Data) {
    throw errCode__default["default"](new Error('Parent node with no data passed to addFileToShardedDirectory'), 'ERR_INVALID_PARENT');
  }
  const rootBucket = await hamtUtils.recreateInitialHamtLevel(options.parent.Links);
  const node = ipfsUnixfs.UnixFS.unmarshal(options.parent.Data);
  const shard = new dirSharded.DirSharded({
    root: true,
    dir: true,
    parent: undefined,
    parentKey: undefined,
    path: '',
    dirty: true,
    flat: false,
    mode: node.mode
  }, options);
  shard._bucket = rootBucket;
  if (node.mtime) {
    shard.mtime = { secs: Math.round(Date.now() / 1000) };
  }
  const position = await rootBucket._findNewBucketAndPos(file.name);
  const path = toBucketPath(position);
  path[0].node = options.parent;
  let index = 0;
  while (index < path.length) {
    const segment = path[index];
    index++;
    const node = segment.node;
    if (!node) {
      throw new Error('Segment had no node');
    }
    const link = node.Links.find(link => (link.Name || '').substring(0, 2) === segment.prefix);
    if (!link) {
      log(`Link ${ segment.prefix }${ file.name } will be added`);
      index = path.length;
      break;
    }
    if (link.Name === `${ segment.prefix }${ file.name }`) {
      log(`Link ${ segment.prefix }${ file.name } will be replaced`);
      index = path.length;
      break;
    }
    if ((link.Name || '').length > 2) {
      log(`Link ${ link.Name } ${ link.Hash } will be replaced with a subshard`);
      index = path.length;
      break;
    }
    log(`Found subshard ${ segment.prefix }`);
    const block = await context.repo.blocks.get(link.Hash);
    const subShard = dagPB__namespace.decode(block);
    if (!path[index]) {
      log(`Loaded new subshard ${ segment.prefix }`);
      await hamtUtils.recreateHamtLevel(context, subShard.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));
      const position = await rootBucket._findNewBucketAndPos(file.name);
      path.push({
        bucket: position.bucket,
        prefix: hamtUtils.toPrefix(position.pos),
        node: subShard
      });
      break;
    }
    const nextSegment = path[index];
    await hamtUtils.addLinksToHamtBucket(context, subShard.Links, nextSegment.bucket, rootBucket);
    nextSegment.node = subShard;
  }
  await shard._bucket.put(file.name, {
    size: file.size,
    cid: file.cid
  });
  return {
    shard,
    path
  };
};
const toBucketPath = position => {
  const path = [{
      bucket: position.bucket,
      prefix: hamtUtils.toPrefix(position.pos)
    }];
  let bucket = position.bucket._parent;
  let positionInBucket = position.bucket._posAtParent;
  while (bucket) {
    path.push({
      bucket,
      prefix: hamtUtils.toPrefix(positionInBucket)
    });
    positionInBucket = bucket._posAtParent;
    bucket = bucket._parent;
  }
  path.reverse();
  return path;
};

exports.addLink = addLink;
