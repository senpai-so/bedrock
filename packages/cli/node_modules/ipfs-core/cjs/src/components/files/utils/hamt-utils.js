'use strict';

Object.defineProperty(exports, '__esModule', { value: true });

var dagPB = require('@ipld/dag-pb');
var hamtSharding = require('hamt-sharding');
var dirSharded = require('./dir-sharded.js');
var debug = require('debug');
var ipfsUnixfs = require('ipfs-unixfs');
var last = require('it-last');
var cid = require('multiformats/cid');
var hamtConstants = require('./hamt-constants.js');

function _interopDefaultLegacy (e) { return e && typeof e === 'object' && 'default' in e ? e : { 'default': e }; }

function _interopNamespace(e) {
  if (e && e.__esModule) return e;
  var n = Object.create(null);
  if (e) {
    Object.keys(e).forEach(function (k) {
      if (k !== 'default') {
        var d = Object.getOwnPropertyDescriptor(e, k);
        Object.defineProperty(n, k, d.get ? d : {
          enumerable: true,
          get: function () { return e[k]; }
        });
      }
    });
  }
  n["default"] = e;
  return Object.freeze(n);
}

var dagPB__namespace = /*#__PURE__*/_interopNamespace(dagPB);
var debug__default = /*#__PURE__*/_interopDefaultLegacy(debug);
var last__default = /*#__PURE__*/_interopDefaultLegacy(last);

const log = debug__default["default"]('ipfs:mfs:core:utils:hamt-utils');
const updateHamtDirectory = async (context, links, bucket, options) => {
  if (!options.parent.Data) {
    throw new Error('Could not update HAMT directory because parent had no data');
  }
  const data = Uint8Array.from(bucket._children.bitField().reverse());
  const node = ipfsUnixfs.UnixFS.unmarshal(options.parent.Data);
  const dir = new ipfsUnixfs.UnixFS({
    type: 'hamt-sharded-directory',
    data,
    fanout: bucket.tableSize(),
    hashType: hamtConstants.hamtHashCode,
    mode: node.mode,
    mtime: node.mtime
  });
  const hasher = await context.hashers.getHasher(options.hashAlg);
  const parent = {
    Data: dir.marshal(),
    Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))
  };
  const buf = dagPB__namespace.encode(parent);
  const hash = await hasher.digest(buf);
  const cid$1 = cid.CID.create(options.cidVersion, dagPB__namespace.code, hash);
  if (options.flush) {
    await context.repo.blocks.put(cid$1, buf);
  }
  return {
    node: parent,
    cid: cid$1,
    size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)
  };
};
const recreateHamtLevel = async (context, links, rootBucket, parentBucket, positionAtParent) => {
  const bucket = new hamtSharding.Bucket({
    hash: rootBucket._options.hash,
    bits: rootBucket._options.bits
  }, parentBucket, positionAtParent);
  parentBucket._putObjectAt(positionAtParent, bucket);
  await addLinksToHamtBucket(context, links, bucket, rootBucket);
  return bucket;
};
const recreateInitialHamtLevel = async links => {
  const bucket = hamtSharding.createHAMT({
    hashFn: hamtConstants.hamtHashFn,
    bits: hamtConstants.hamtBucketBits
  });
  await Promise.all(links.map(async link => {
    const linkName = link.Name || '';
    if (linkName.length === 2) {
      const pos = parseInt(linkName, 16);
      const subBucket = new hamtSharding.Bucket({
        hash: bucket._options.hash,
        bits: bucket._options.bits
      }, bucket, pos);
      bucket._putObjectAt(pos, subBucket);
      return Promise.resolve();
    }
    return bucket.put(linkName.substring(2), {
      size: link.Tsize,
      cid: link.Hash
    });
  }));
  return bucket;
};
const addLinksToHamtBucket = async (context, links, bucket, rootBucket) => {
  await Promise.all(links.map(async link => {
    const linkName = link.Name || '';
    if (linkName.length === 2) {
      log('Populating sub bucket', linkName);
      const pos = parseInt(linkName, 16);
      const block = await context.repo.blocks.get(link.Hash);
      const node = dagPB__namespace.decode(block);
      const subBucket = new hamtSharding.Bucket({
        hash: rootBucket._options.hash,
        bits: rootBucket._options.bits
      }, bucket, pos);
      bucket._putObjectAt(pos, subBucket);
      await addLinksToHamtBucket(context, node.Links, subBucket, rootBucket);
      return Promise.resolve();
    }
    return rootBucket.put(linkName.substring(2), {
      size: link.Tsize,
      cid: link.Hash
    });
  }));
};
const toPrefix = position => {
  return position.toString(16).toUpperCase().padStart(2, '0').substring(0, 2);
};
const generatePath = async (context, fileName, rootNode) => {
  const rootBucket = await recreateInitialHamtLevel(rootNode.Links);
  const position = await rootBucket._findNewBucketAndPos(fileName);
  const path = [{
      bucket: position.bucket,
      prefix: toPrefix(position.pos)
    }];
  let currentBucket = position.bucket;
  while (currentBucket !== rootBucket) {
    path.push({
      bucket: currentBucket,
      prefix: toPrefix(currentBucket._posAtParent)
    });
    currentBucket = currentBucket._parent;
  }
  path.reverse();
  path[0].node = rootNode;
  for (let i = 0; i < path.length; i++) {
    const segment = path[i];
    if (!segment.node) {
      throw new Error('Could not generate HAMT path');
    }
    const link = segment.node.Links.filter(link => (link.Name || '').substring(0, 2) === segment.prefix).pop();
    if (!link) {
      log(`Link ${ segment.prefix }${ fileName } will be added`);
      continue;
    }
    if (link.Name === `${ segment.prefix }${ fileName }`) {
      log(`Link ${ segment.prefix }${ fileName } will be replaced`);
      continue;
    }
    log(`Found subshard ${ segment.prefix }`);
    const block = await context.repo.blocks.get(link.Hash);
    const node = dagPB__namespace.decode(block);
    if (!path[i + 1]) {
      log(`Loaded new subshard ${ segment.prefix }`);
      await recreateHamtLevel(context, node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));
      const position = await rootBucket._findNewBucketAndPos(fileName);
      path.push({
        bucket: position.bucket,
        prefix: toPrefix(position.pos),
        node: node
      });
      continue;
    }
    const nextSegment = path[i + 1];
    await addLinksToHamtBucket(context, node.Links, nextSegment.bucket, rootBucket);
    nextSegment.node = node;
  }
  await rootBucket.put(fileName, true);
  path.reverse();
  return {
    rootBucket,
    path
  };
};
const createShard = async (context, contents, options = {}) => {
  const shard = new dirSharded.DirSharded({
    root: true,
    dir: true,
    parent: undefined,
    parentKey: undefined,
    path: '',
    dirty: true,
    flat: false,
    mtime: options.mtime,
    mode: options.mode
  }, options);
  for (let i = 0; i < contents.length; i++) {
    await shard._bucket.put(contents[i].name, {
      size: contents[i].size,
      cid: contents[i].cid
    });
  }
  const res = await last__default["default"](shard.flush(context.repo.blocks));
  if (!res) {
    throw new Error('Flushing shard yielded no result');
  }
  return res;
};

exports.addLinksToHamtBucket = addLinksToHamtBucket;
exports.createShard = createShard;
exports.generatePath = generatePath;
exports.recreateHamtLevel = recreateHamtLevel;
exports.recreateInitialHamtLevel = recreateInitialHamtLevel;
exports.toPrefix = toPrefix;
exports.updateHamtDirectory = updateHamtDirectory;
