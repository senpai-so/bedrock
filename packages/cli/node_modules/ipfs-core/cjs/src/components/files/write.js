'use strict';

Object.defineProperty(exports, '__esModule', { value: true });

var debug = require('debug');
var ipfsUnixfsImporter = require('ipfs-unixfs-importer');
var dagPB = require('@ipld/dag-pb');
var stat = require('./stat.js');
var mkdir = require('./mkdir.js');
var addLink = require('./utils/add-link.js');
var mergeOpts = require('merge-options');
var createLock = require('./utils/create-lock.js');
var toAsyncIterator = require('./utils/to-async-iterator.js');
var toMfsPath = require('./utils/to-mfs-path.js');
var toPathComponents = require('./utils/to-path-components.js');
var toTrail = require('./utils/to-trail.js');
var updateTree = require('./utils/update-tree.js');
var updateMfsRoot = require('./utils/update-mfs-root.js');
var errCode = require('err-code');
var utils = require('../../utils.js');
var last = require('it-last');
var withTimeoutOption = require('ipfs-core-utils/with-timeout-option');
var ipfsUnixfs = require('ipfs-unixfs');

function _interopDefaultLegacy (e) { return e && typeof e === 'object' && 'default' in e ? e : { 'default': e }; }

var debug__default = /*#__PURE__*/_interopDefaultLegacy(debug);
var mergeOpts__default = /*#__PURE__*/_interopDefaultLegacy(mergeOpts);
var errCode__default = /*#__PURE__*/_interopDefaultLegacy(errCode);
var last__default = /*#__PURE__*/_interopDefaultLegacy(last);

const mergeOptions = mergeOpts__default["default"].bind({ ignoreUndefined: true });
const log = debug__default["default"]('ipfs:mfs:write');
const defaultOptions = {
  offset: 0,
  length: Infinity,
  create: false,
  truncate: false,
  rawLeaves: false,
  reduceSingleLeafToSelf: false,
  cidVersion: 0,
  hashAlg: 'sha2-256',
  parents: false,
  progress: (bytes, path) => {
  },
  strategy: 'trickle',
  flush: true,
  leafType: 'raw',
  shardSplitThreshold: 1000
};
function createWrite(context) {
  async function mfsWrite(path, content, opts = {}) {
    const options = mergeOptions(defaultOptions, opts);
    let source;
    let destination;
    let parent;
    log('Reading source, destination and parent');
    await createLock.createLock().readLock(async () => {
      source = await toAsyncIterator.toAsyncIterator(content);
      destination = await toMfsPath.toMfsPath(context, path, options);
      parent = await toMfsPath.toMfsPath(context, destination.mfsDirectory, options);
    })();
    log('Read source, destination and parent');
    if (!options.parents && !parent.exists) {
      throw errCode__default["default"](new Error('directory does not exist'), 'ERR_NO_EXIST');
    }
    if (source == null) {
      throw errCode__default["default"](new Error('could not create source'), 'ERR_NO_SOURCE');
    }
    if (destination == null) {
      throw errCode__default["default"](new Error('could not create destination'), 'ERR_NO_DESTINATION');
    }
    if (!options.create && !destination.exists) {
      throw errCode__default["default"](new Error('file does not exist'), 'ERR_NO_EXIST');
    }
    if (destination.entryType !== 'file') {
      throw errCode__default["default"](new Error('not a file'), 'ERR_NOT_A_FILE');
    }
    return updateOrImport(context, path, source, destination, options);
  }
  return withTimeoutOption.withTimeoutOption(mfsWrite);
}
const updateOrImport = async (context, path, source, destination, options) => {
  const child = await write(context, source, destination, options);
  await createLock.createLock().writeLock(async () => {
    const pathComponents = toPathComponents.toPathComponents(path);
    const fileName = pathComponents.pop();
    if (fileName == null) {
      throw errCode__default["default"](new Error('source does not exist'), 'ERR_NO_EXIST');
    }
    let parentExists = false;
    try {
      await stat.createStat(context)(`/${ pathComponents.join('/') }`, options);
      parentExists = true;
    } catch (err) {
      if (err.code !== 'ERR_NOT_FOUND') {
        throw err;
      }
    }
    if (!parentExists) {
      await mkdir.createMkdir(context)(`/${ pathComponents.join('/') }`, options);
    }
    const updatedPath = await toMfsPath.toMfsPath(context, path, options);
    const trail = await toTrail.toTrail(context, updatedPath.mfsDirectory);
    const parent = trail[trail.length - 1];
    if (!parent) {
      throw errCode__default["default"](new Error('directory does not exist'), 'ERR_NO_EXIST');
    }
    if (!parent.type || !parent.type.includes('directory')) {
      throw errCode__default["default"](new Error(`cannot write to ${ parent.name }: Not a directory`), 'ERR_NOT_A_DIRECTORY');
    }
    const parentBlock = await context.repo.blocks.get(parent.cid);
    const parentNode = dagPB.decode(parentBlock);
    const result = await addLink.addLink(context, {
      parent: parentNode,
      name: fileName,
      cid: child.cid,
      size: child.size,
      flush: options.flush,
      shardSplitThreshold: options.shardSplitThreshold,
      hashAlg: options.hashAlg,
      cidVersion: options.cidVersion
    });
    parent.cid = result.cid;
    const newRootCid = await updateTree.updateTree(context, trail, options);
    await updateMfsRoot.updateMfsRoot(context, newRootCid, options);
  })();
};
const write = async (context, source, destination, options) => {
  if (destination.exists) {
    log(`Overwriting file ${ destination.cid } offset ${ options.offset } length ${ options.length }`);
  } else {
    log(`Writing file offset ${ options.offset } length ${ options.length }`);
  }
  const sources = [];
  if (options.offset > 0) {
    if (destination.unixfs) {
      log(`Writing first ${ options.offset } bytes of original file`);
      sources.push(() => {
        return destination.content({
          offset: 0,
          length: options.offset
        });
      });
      if (destination.unixfs.fileSize() < options.offset) {
        const extra = options.offset - destination.unixfs.fileSize();
        log(`Writing zeros for extra ${ extra } bytes`);
        sources.push(asyncZeroes(extra));
      }
    } else {
      log(`Writing zeros for first ${ options.offset } bytes`);
      sources.push(asyncZeroes(options.offset));
    }
  }
  sources.push(limitAsyncStreamBytes(source, options.length));
  const content = countBytesStreamed(catAsyncIterators(sources), bytesWritten => {
    if (destination.unixfs && !options.truncate) {
      const fileSize = destination.unixfs.fileSize();
      if (fileSize > bytesWritten) {
        log(`Writing last ${ fileSize - bytesWritten } of ${ fileSize } bytes from original file starting at offset ${ bytesWritten }`);
        return destination.content({ offset: bytesWritten });
      } else {
        log('Not writing last bytes from original file');
      }
    }
    return {
      [Symbol.asyncIterator]: async function* () {
      }
    };
  });
  let mode;
  if (options.mode !== undefined && options.mode !== null) {
    mode = ipfsUnixfs.parseMode(options.mode);
  } else if (destination && destination.unixfs) {
    mode = destination.unixfs.mode;
  }
  let mtime;
  if (options.mtime != null) {
    mtime = ipfsUnixfs.parseMtime(options.mtime);
  } else if (destination && destination.unixfs) {
    mtime = destination.unixfs.mtime;
  }
  const hasher = await context.hashers.getHasher(options.hashAlg);
  const result = await last__default["default"](ipfsUnixfsImporter.importer([{
      content: content,
      mode,
      mtime
    }], context.repo.blocks, {
    progress: options.progress,
    hasher,
    cidVersion: options.cidVersion,
    strategy: options.strategy,
    rawLeaves: options.rawLeaves,
    reduceSingleLeafToSelf: options.reduceSingleLeafToSelf,
    leafType: options.leafType
  }));
  if (!result) {
    throw errCode__default["default"](new Error(`cannot write to ${ parent.name }`), 'ERR_COULD_NOT_WRITE');
  }
  log(`Wrote ${ result.cid }`);
  return {
    cid: result.cid,
    size: result.size
  };
};
const limitAsyncStreamBytes = (stream, limit) => {
  return async function* _limitAsyncStreamBytes() {
    let emitted = 0;
    for await (const buf of stream) {
      emitted += buf.length;
      if (emitted > limit) {
        yield buf.slice(0, limit - emitted);
        return;
      }
      yield buf;
    }
  };
};
const asyncZeroes = (count, chunkSize = utils.MFS_MAX_CHUNK_SIZE) => {
  const buf = new Uint8Array(chunkSize);
  async function* _asyncZeroes() {
    while (true) {
      yield buf.slice();
    }
  }
  return limitAsyncStreamBytes(_asyncZeroes(), count);
};
const catAsyncIterators = async function* (sources) {
  for (let i = 0; i < sources.length; i++) {
    yield* sources[i]();
  }
};
const countBytesStreamed = async function* (source, notify) {
  let wrote = 0;
  for await (const buf of source) {
    wrote += buf.length;
    yield buf;
  }
  for await (const buf of notify(wrote)) {
    wrote += buf.length;
    yield buf;
  }
};

exports.createWrite = createWrite;
