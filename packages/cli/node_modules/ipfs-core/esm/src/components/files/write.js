import debug from 'debug';
import { importer } from 'ipfs-unixfs-importer';
import { decode } from '@ipld/dag-pb';
import { createStat } from './stat.js';
import { createMkdir } from './mkdir.js';
import { addLink } from './utils/add-link.js';
import mergeOpts from 'merge-options';
import { createLock } from './utils/create-lock.js';
import { toAsyncIterator } from './utils/to-async-iterator.js';
import { toMfsPath } from './utils/to-mfs-path.js';
import { toPathComponents } from './utils/to-path-components.js';
import { toTrail } from './utils/to-trail.js';
import { updateTree } from './utils/update-tree.js';
import { updateMfsRoot } from './utils/update-mfs-root.js';
import errCode from 'err-code';
import { MFS_MAX_CHUNK_SIZE } from '../../utils.js';
import last from 'it-last';
import { withTimeoutOption } from 'ipfs-core-utils/with-timeout-option';
import {
  parseMode,
  parseMtime
} from 'ipfs-unixfs';
const mergeOptions = mergeOpts.bind({ ignoreUndefined: true });
const log = debug('ipfs:mfs:write');
const defaultOptions = {
  offset: 0,
  length: Infinity,
  create: false,
  truncate: false,
  rawLeaves: false,
  reduceSingleLeafToSelf: false,
  cidVersion: 0,
  hashAlg: 'sha2-256',
  parents: false,
  progress: (bytes, path) => {
  },
  strategy: 'trickle',
  flush: true,
  leafType: 'raw',
  shardSplitThreshold: 1000
};
export function createWrite(context) {
  async function mfsWrite(path, content, opts = {}) {
    const options = mergeOptions(defaultOptions, opts);
    let source;
    let destination;
    let parent;
    log('Reading source, destination and parent');
    await createLock().readLock(async () => {
      source = await toAsyncIterator(content);
      destination = await toMfsPath(context, path, options);
      parent = await toMfsPath(context, destination.mfsDirectory, options);
    })();
    log('Read source, destination and parent');
    if (!options.parents && !parent.exists) {
      throw errCode(new Error('directory does not exist'), 'ERR_NO_EXIST');
    }
    if (source == null) {
      throw errCode(new Error('could not create source'), 'ERR_NO_SOURCE');
    }
    if (destination == null) {
      throw errCode(new Error('could not create destination'), 'ERR_NO_DESTINATION');
    }
    if (!options.create && !destination.exists) {
      throw errCode(new Error('file does not exist'), 'ERR_NO_EXIST');
    }
    if (destination.entryType !== 'file') {
      throw errCode(new Error('not a file'), 'ERR_NOT_A_FILE');
    }
    return updateOrImport(context, path, source, destination, options);
  }
  return withTimeoutOption(mfsWrite);
}
const updateOrImport = async (context, path, source, destination, options) => {
  const child = await write(context, source, destination, options);
  await createLock().writeLock(async () => {
    const pathComponents = toPathComponents(path);
    const fileName = pathComponents.pop();
    if (fileName == null) {
      throw errCode(new Error('source does not exist'), 'ERR_NO_EXIST');
    }
    let parentExists = false;
    try {
      await createStat(context)(`/${ pathComponents.join('/') }`, options);
      parentExists = true;
    } catch (err) {
      if (err.code !== 'ERR_NOT_FOUND') {
        throw err;
      }
    }
    if (!parentExists) {
      await createMkdir(context)(`/${ pathComponents.join('/') }`, options);
    }
    const updatedPath = await toMfsPath(context, path, options);
    const trail = await toTrail(context, updatedPath.mfsDirectory);
    const parent = trail[trail.length - 1];
    if (!parent) {
      throw errCode(new Error('directory does not exist'), 'ERR_NO_EXIST');
    }
    if (!parent.type || !parent.type.includes('directory')) {
      throw errCode(new Error(`cannot write to ${ parent.name }: Not a directory`), 'ERR_NOT_A_DIRECTORY');
    }
    const parentBlock = await context.repo.blocks.get(parent.cid);
    const parentNode = decode(parentBlock);
    const result = await addLink(context, {
      parent: parentNode,
      name: fileName,
      cid: child.cid,
      size: child.size,
      flush: options.flush,
      shardSplitThreshold: options.shardSplitThreshold,
      hashAlg: options.hashAlg,
      cidVersion: options.cidVersion
    });
    parent.cid = result.cid;
    const newRootCid = await updateTree(context, trail, options);
    await updateMfsRoot(context, newRootCid, options);
  })();
};
const write = async (context, source, destination, options) => {
  if (destination.exists) {
    log(`Overwriting file ${ destination.cid } offset ${ options.offset } length ${ options.length }`);
  } else {
    log(`Writing file offset ${ options.offset } length ${ options.length }`);
  }
  const sources = [];
  if (options.offset > 0) {
    if (destination.unixfs) {
      log(`Writing first ${ options.offset } bytes of original file`);
      sources.push(() => {
        return destination.content({
          offset: 0,
          length: options.offset
        });
      });
      if (destination.unixfs.fileSize() < options.offset) {
        const extra = options.offset - destination.unixfs.fileSize();
        log(`Writing zeros for extra ${ extra } bytes`);
        sources.push(asyncZeroes(extra));
      }
    } else {
      log(`Writing zeros for first ${ options.offset } bytes`);
      sources.push(asyncZeroes(options.offset));
    }
  }
  sources.push(limitAsyncStreamBytes(source, options.length));
  const content = countBytesStreamed(catAsyncIterators(sources), bytesWritten => {
    if (destination.unixfs && !options.truncate) {
      const fileSize = destination.unixfs.fileSize();
      if (fileSize > bytesWritten) {
        log(`Writing last ${ fileSize - bytesWritten } of ${ fileSize } bytes from original file starting at offset ${ bytesWritten }`);
        return destination.content({ offset: bytesWritten });
      } else {
        log('Not writing last bytes from original file');
      }
    }
    return {
      [Symbol.asyncIterator]: async function* () {
      }
    };
  });
  let mode;
  if (options.mode !== undefined && options.mode !== null) {
    mode = parseMode(options.mode);
  } else if (destination && destination.unixfs) {
    mode = destination.unixfs.mode;
  }
  let mtime;
  if (options.mtime != null) {
    mtime = parseMtime(options.mtime);
  } else if (destination && destination.unixfs) {
    mtime = destination.unixfs.mtime;
  }
  const hasher = await context.hashers.getHasher(options.hashAlg);
  const result = await last(importer([{
      content: content,
      mode,
      mtime
    }], context.repo.blocks, {
    progress: options.progress,
    hasher,
    cidVersion: options.cidVersion,
    strategy: options.strategy,
    rawLeaves: options.rawLeaves,
    reduceSingleLeafToSelf: options.reduceSingleLeafToSelf,
    leafType: options.leafType
  }));
  if (!result) {
    throw errCode(new Error(`cannot write to ${ parent.name }`), 'ERR_COULD_NOT_WRITE');
  }
  log(`Wrote ${ result.cid }`);
  return {
    cid: result.cid,
    size: result.size
  };
};
const limitAsyncStreamBytes = (stream, limit) => {
  return async function* _limitAsyncStreamBytes() {
    let emitted = 0;
    for await (const buf of stream) {
      emitted += buf.length;
      if (emitted > limit) {
        yield buf.slice(0, limit - emitted);
        return;
      }
      yield buf;
    }
  };
};
const asyncZeroes = (count, chunkSize = MFS_MAX_CHUNK_SIZE) => {
  const buf = new Uint8Array(chunkSize);
  async function* _asyncZeroes() {
    while (true) {
      yield buf.slice();
    }
  }
  return limitAsyncStreamBytes(_asyncZeroes(), count);
};
const catAsyncIterators = async function* (sources) {
  for (let i = 0; i < sources.length; i++) {
    yield* sources[i]();
  }
};
const countBytesStreamed = async function* (source, notify) {
  let wrote = 0;
  for await (const buf of source) {
    wrote += buf.length;
    yield buf;
  }
  for await (const buf of notify(wrote)) {
    wrote += buf.length;
    yield buf;
  }
};